{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales: Vale, ¿pero de dónde han salido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Las redes neuronales están muy de moda ahora, pero para entenderlas de verdad, tenemos que ver su origen. Y su origen, por antiintuitivo que resulte dada su reciente popularidad,\n",
    "está muy atrás, datando de los orígenes del mismísimo Machine Learning.\n",
    "\n",
    "Es 1943. Aparece el primer modelo de neurona artificial, la **neurona McCulloch-Pitts (MCP)**, planteada simplemente como una puerta lógica que recibe estímulos eléctricos.\n",
    "O se dispara o no; o 1 ó 0. Unos cuantos años más tarde, aparece el primer modelo en hacer uso de este concepto de neurona para aprender, en base a una entrada de \"features\",\n",
    "los coeficientes que debía aprender el algoritmo para decidir si una neurona se activaba o no.\n",
    "\n",
    "Este modelo es conocido como el **perceptrón**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un **vector** $X$ de features, que serán la entrada del algoritmo, y otro vector $W$ de **pesos**, uno por cada feature:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_{1}\\\\ \n",
    "x_{2}\\\\ \n",
    "...\\\\ \n",
    "x_{n}\n",
    "\\end{bmatrix}\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "W = \\begin{bmatrix}\n",
    "w_{1}\\\\ \n",
    "w_{2}\\\\ \n",
    "...\\\\ \n",
    "w_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Sobre estos dos vectores aplicaremos una combinación lineal, a la que denominaremos **net input** ó $z$, que no es más que el **producto punto** de $X$ y $W$:\n",
    "\n",
    "$z = x_{1}w_{1} + x_{2}w_{2} + ... + x_{n}w_{n}$\n",
    "\n",
    "Dado este **net input** $z$, para un problema de clasificación binaria, donde solo queremos predecir si una variable es de clase 0 ó 1, definimos una **función de decisión**  $\\sigma(z)$ muy sencilla:\n",
    "\n",
    "$\\sigma(z) = \\begin{cases}\n",
    "0 & \\text{si } z \\geq \\theta  \\\\ \n",
    "1 & \\text{en otro caso}\n",
    "\\end{cases}$\n",
    "\n",
    "Es decir: si el **net input** entre los vectores $X$ y $W$ es mayor o igual que un determinado valor $\\theta$ que escojamos, diremos que ese ejemplo es de clase 1. De lo contrario, será de\n",
    "clase 0.\n",
    "\n",
    "Para simplificar esta función de decisión, definiremos una **unidad de sesgo** o *bias* $b = -\\theta$, y la añadiremos al net input, de tal manera que se nos quedará así:\n",
    "\n",
    "$z = x_{1}w_{1} + x_{2}w_{2} + ... + x_{n}w_{n} + b$\n",
    "\n",
    "Con esta nueva definición de $z$, podemos dejar la función de decisión $\\sigma$ tal que así:\n",
    "\n",
    "$\\sigma(z) = \\begin{cases}\n",
    "0 & \\text{si } z \\geq 0  \\\\ \n",
    "1 & \\text{en otro caso}\n",
    "\\end{cases}$\n",
    "\n",
    "Todo esto está muy bien, pero hasta ahora solo hemos establecido definiciones. Los valores de $X$ nos son dados, pero ¿cómo podemos averiguar los valores de $W$ que consigan discriminar una clase de la otra?\n",
    "\n",
    "Aquí entra el algoritmo del **perceptrón**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia, vamos a inicializar el vector $W$ a valores pequeños aleatorios, los que sean.\n",
    "\n",
    "A continuación, a lo largo de $N$ iteraciones, a nuestro perceptrón le vamos a pasar un conjunto $T$ de vectores $X$ tal como los hemos definido anteriormente. Por cada vector $X$ de $T$ (es decir, por cada registro que tengamos para nuestro entrenamiento), vamos a computar a qué clase pertenecería según nuestro vector $W$. Dicha predicción puede ser correcta o errónea.\n",
    "\n",
    "Aquí es donde sucede la magia. Nuestro perceptrón comparará la predicción con la realidad, y actualizará los pesos de $W$ de manera acorde. Concretamente, para cada elemento $w_{j}$ de $W$:\n",
    "\n",
    "$w_{j} := w_{j} + \\Delta{w} \\\\\n",
    "b := b + \\Delta{b}$,\n",
    "\n",
    "donde $\\Delta{w}$ y  $\\Delta{b}$ se definen como:\n",
    "\n",
    "$\\Delta{w} = x_{j}\\eta(\\hat{y_{j}} - y_{j}) \\\\\n",
    "\\Delta{b} = \\eta(\\hat{y_{j}} - y_{j})$,\n",
    "\n",
    "donde $y_{j}$ es el valor **real**, $\\hat{y_{j}}$ es nuestro valor **predicho** para ese registro, y $\\eta$ es el **learning rate**, que determina la velocidad a la que aprenderá nuestro algoritmo.\n",
    "\n",
    "Es importante notar que no hemos definido el número $N$ de iteraciones que haremos sobre nuestro conjunto de vectores. Esta $N$ debe ser un número finito. Esto se debe a que este algoritmo no convergerá nunca a cero errores para conjuntos de datos no separables linealmente. Cada iteración de las $N$ que definamos se denominan **épocas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es una mejora sobre el perceptrón, con una diferencia (o más bien añadido) fundamental: en vez de computar la actualización de los pesos en base a la predicción de cada registro, lo haremos en base a una **función de activación**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
